{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Convert Detectron2 Models to OpenVINO™\n",
    "\n",
    "[Detectron2](https://github.com/facebookresearch/detectron2) is Facebook AI Research's library that provides state-of-the-art detection and segmentation algorithms. It is the successor of [Detectron](https://github.com/facebookresearch/Detectron/) and [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/). It supports a number of computer vision research projects and production applications. \n",
    "\n",
    "In this tutorial we consider how to convert and run Detectron2 models using OpenVINO™. We will use `Faster R-CNN FPN x1` model and `Mask R-CNN FPN x3` pretrained on [COCO](https://cocodataset.org/#home) dataset as examples for object detection and instance segmentation respectively.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "    - [Define helpers for PyTorch model initialization and conversion](#Define-helpers-for-PyTorch-model-initialization-and-conversion)\n",
    "    - [Prepare input data](#Prepare-input-data)\n",
    "- [Object Detection](#Object-Detection)\n",
    "    - [Download PyTorch Detection model](#Download-PyTorch-Detection-model)\n",
    "    - [Convert Detection Model to OpenVINO Intermediate Representation](#Convert-Detection-Model-to-OpenVINO-Intermediate-Representation)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Run Detection model inference](#Run-Detection-model-inference)\n",
    "- [Instance Segmentation](#Instance-Segmentation)\n",
    "    - [Download Instance Segmentation PyTorch model](#Download-Instance-Segmentation-PyTorch-model)\n",
    "    - [Convert Instance Segmentation Model to OpenVINO Intermediate Representation](#Convert-Instance-Segmentation-Model-to-OpenVINO-Intermediate-Representation)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Run Instance Segmentation model inference](#Run-Instance-Segmentation-model-inference)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/detectron2-to-openvino/detectron2-to-openvino.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install required packages for running model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "    )\n",
    "    with open(\"notebook_utils.py\", \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "if not Path(\"pip_helper.py\").exists():\n",
    "    r = requests.get(\n",
    "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/pip_helper.py\",\n",
    "    )\n",
    "    open(\"pip_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from pip_helper import pip_install\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    pip_install(\"numpy<2.0.0\")\n",
    "pip_install(\"torch\", \"torchvision\", \"opencv-python\", \"wheel\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cpu\")\n",
    "pip_install(\"git+https://github.com/facebookresearch/detectron2.git\", \"--extra-index-url\", \"https://download.pytorch.org/whl/cpu\")\n",
    "pip_install(\"openvino>=2023.1.0\")\n",
    "\n",
    "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
    "from notebook_utils import collect_telemetry\n",
    "\n",
    "collect_telemetry(\"detectron2-to-openvino.ipynb\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helpers for PyTorch model initialization and conversion\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Detectron2 provides universal and configurable API for working with models, it means that all steps required for model creation, conversion and inference will be common for all models, that is why it is enough to define helper functions once, then reuse them for different models.\n",
    "For obtaining models we will use [Detectron2 Model Zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md) API. `detecton_zoo.get` function allow to download and instantiate model based on its config file. Configuration file is playing key role in interaction with models in Detectron2 project and describes model architecture and training and validation processes. `detectron_zoo.get_config` function can be used for finding and reading model config."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import detectron2.model_zoo as detectron_zoo\n",
    "\n",
    "\n",
    "def get_model_and_config(model_name: str):\n",
    "    \"\"\"\n",
    "    Helper function for downloading PyTorch model and its configuration from Detectron2 Model Zoo\n",
    "\n",
    "    Parameters:\n",
    "      model_name (str): model_id from Detectron2 Model Zoo\n",
    "    Returns:\n",
    "      model (torch.nn.Module): Pretrained model instance\n",
    "      cfg (Config): Configuration for model\n",
    "    \"\"\"\n",
    "    cfg = detectron_zoo.get_config(model_name + \".yaml\", trained=True)\n",
    "    model = detectron_zoo.get(model_name + \".yaml\", trained=True)\n",
    "    return model, cfg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectron2 library is based on PyTorch. Starting from 2023.0 release OpenVINO supports PyTorch models conversion directly via Model Conversion API. `ov.convert_model` function can be used for converting PyTorch model to OpenVINO Model object instance, that ready to use for loading on device and then running inference or can be saved on disk for next deployment using `ov.save_model` function.\n",
    "\n",
    "Detectron2 models use custom complex data structures inside that brings some difficulties for exporting models in different formats and frameworks including OpenVINO. For avoid these issues, `detectron2.export.TracingAdapter` provided as part of Detectron2 deployment API. `TracingAdapter` is a model wrapper class that simplify model's structure making it more export-friendly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from detectron2.modeling import GeneralizedRCNN\n",
    "from detectron2.export import TracingAdapter\n",
    "import torch\n",
    "import openvino as ov\n",
    "from typing import List,Dict\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "def convert_detectron2_model(model: torch.nn.Module, sample_input: List[Dict[str, torch.Tensor]]):\n",
    "    \"\"\"\n",
    "    Function for converting Detectron2 models, creates TracingAdapter for making model tracing-friendly,\n",
    "    prepares inputs and converts model to OpenVINO Model\n",
    "\n",
    "    Parameters:\n",
    "      model (torch.nn.Module): Model object for conversion\n",
    "      sample_input (List[Dict[str, torch.Tensor]]): sample input for tracing\n",
    "    Returns:\n",
    "      ov_model (ov.Model): OpenVINO Model\n",
    "    \"\"\"\n",
    "    # prepare input for tracing adapter\n",
    "    tracing_input = [{\"image\": sample_input[0][\"image\"]}]\n",
    "\n",
    "    # override model forward and disable postprocessing if required\n",
    "    if isinstance(model, GeneralizedRCNN):\n",
    "\n",
    "        def inference(model, inputs):\n",
    "            # use do_postprocess=False so it returns ROI mask\n",
    "            inst = model.inference(inputs, do_postprocess=False)[0]\n",
    "            return [{\"instances\": inst}]\n",
    "\n",
    "    else:\n",
    "        inference = None  # assume that we just call the model directly\n",
    "\n",
    "    # create traceable model\n",
    "    traceable_model = TracingAdapter(model, tracing_input, inference)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # convert PyTorch model to OpenVINO model\n",
    "    ov_model = ov.convert_model(traceable_model, example_input=sample_input[0][\"image\"])\n",
    "    return ov_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "For running model conversion and inference we need to provide example input. The cells below download sample image and apply preprocessing steps based on model specific transformations defined in model config."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "DATA_DIR = Path(\"images\")\n",
    "\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# input_image_url = \"https://farm9.staticflickr.com/8040/8017130856_1b46b5f5fc_z.jpg\"\n",
    "\n",
    "# image_file = DATA_DIR / \"example_image.jpg\"\n",
    "\n",
    "# if not image_file.exists():\n",
    "#     image = Image.open(requests.get(input_image_url, stream=True).raw)\n",
    "#     image.save(image_file)\n",
    "# else:\n",
    "#     image = Image.open(image_file)\n",
    "\n",
    "#use local images\n",
    "image_file = \"images/test1.jpg\"\n",
    "image = Image.open(image_file)\n",
    "\n",
    "image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_sample_inputs(image_path, cfg):\n",
    "    # get a sample data\n",
    "    original_image = detection_utils.read_image(image_path, format=cfg.INPUT.FORMAT)\n",
    "    # Do same preprocessing as DefaultPredictor\n",
    "    aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
    "    height, width = original_image.shape[:2]\n",
    "    image = aug.get_transform(original_image).apply_image(original_image)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "    inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "\n",
    "    # Sample ready\n",
    "    sample_inputs = [inputs]\n",
    "    return sample_inputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when all components required for model conversion are prepared, we can consider how to use them on specific examples.\n",
    "\n",
    "## Object Detection\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PyTorch Detection model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Download faster_rcnn_R_50_FPN_1x from Detectron Model Zoo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_name = \"COCO-Detection/faster_rcnn_R_50_FPN_1x\"\n",
    "model, cfg = get_model_and_config(model_name)\n",
    "sample_input = get_sample_inputs(image_file, cfg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Detection Model to OpenVINO Intermediate Representation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Convert model using `convert_detectron2_model` function and `sample_input` prepared above. After conversion, model saved on disk using `ov.save_model` function and can be found in `model` directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_xml_path = MODEL_DIR / (model_name.split(\"/\")[-1] + \".xml\")\n",
    "if not model_xml_path.exists():\n",
    "    ov_model = convert_detectron2_model(model, sample_input)\n",
    "    ov.save_model(ov_model, MODEL_DIR / (model_name.split(\"/\")[-1] + \".xml\"))\n",
    "else:\n",
    "    ov_model = model_xml_path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Detection model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load our converted model on selected device and run inference on sample input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compiled_model = core.compile_model(ov_model, device.value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = compiled_model(sample_input[0][\"image\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing adapter simplifies model input and output format. After conversion, model has multiple outputs in following format:\n",
    "1. Predicted boxes is floating-point tensor in format [`N`, 4], where N is number of detected boxes.\n",
    "2. Predicted classes is integer tensor in format [`N`], where N is number of predicted objects that defines which label each object belongs. The values range of predicted classes tensor is [0, `num_labels`], where `num_labels` is number of classes supported of model (in our case 80).\n",
    "3. Predicted scores is floating-point tensor in format [`N`], where `N` is number of predicted objects that defines confidence of each prediction.\n",
    "4. Input image size is integer tensor with values [`H`, `W`], where `H` is height of input data and `W` is width of input data, used for rescaling predictions on postprocessing step.\n",
    "\n",
    "For reusing Detectron2 API for postprocessing and visualization, we provide helpers for wrapping output in original Detectron2 format."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from detectron2.structures import Instances, Boxes\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_detection_result(outputs: dict, orig_height: int, orig_width: int, conf_threshold: float = 0.0):\n",
    "    \"\"\"\n",
    "    Helper function for postprocessing prediction results\n",
    "\n",
    "    Parameters:\n",
    "      outputs (Dict): OpenVINO model output dictionary\n",
    "      orig_height (int): original image height before preprocessing\n",
    "      orig_width (int): original image width before preprocessing\n",
    "      conf_threshold (float, optional, defaults 0.0): confidence threshold for valid prediction\n",
    "    Returns:\n",
    "      prediction_result (instances): postprocessed predicted instances\n",
    "    \"\"\"\n",
    "    boxes = outputs[0]\n",
    "    classes = outputs[1]\n",
    "    has_mask = len(outputs) >= 5\n",
    "    masks = None if not has_mask else outputs[2]\n",
    "    scores = outputs[2 if not has_mask else 3]\n",
    "    model_input_size = (\n",
    "        int(outputs[3 if not has_mask else 4][0]),\n",
    "        int(outputs[3 if not has_mask else 4][1]),\n",
    "    )\n",
    "    filtered_detections = scores >= conf_threshold\n",
    "    boxes = Boxes(boxes[filtered_detections])\n",
    "    scores = scores[filtered_detections]\n",
    "    classes = classes[filtered_detections]\n",
    "    out_dict = {\"pred_boxes\": boxes, \"scores\": scores, \"pred_classes\": classes}\n",
    "    if masks is not None:\n",
    "        masks = masks[filtered_detections]\n",
    "        out_dict[\"pred_masks\"] = torch.from_numpy(masks)\n",
    "    instances = Instances(model_input_size, **out_dict)\n",
    "    return detector_postprocess(instances, orig_height, orig_width)\n",
    "\n",
    "\n",
    "def draw_instance_prediction(img: np.ndarray, results: Instances, cfg: \"Config\"):\n",
    "    \"\"\"\n",
    "    Helper function for visualization prediction results\n",
    "\n",
    "    Parameters:\n",
    "      img (np.ndarray): original image for drawing predictions\n",
    "      results (instances): model predictions\n",
    "      cfg (Config): model configuration\n",
    "    Returns:\n",
    "       img_with_res: image with results\n",
    "    \"\"\"\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "    visualizer = Visualizer(img, metadata, instance_mode=ColorMode.IMAGE)\n",
    "    img_with_res = visualizer.draw_instance_predictions(results)\n",
    "    return img_with_res"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = postprocess_detection_result(results, sample_input[0][\"height\"], sample_input[0][\"width\"], conf_threshold=0.05)\n",
    "img_with_res = draw_instance_prediction(np.array(image), results, cfg)\n",
    "Image.fromarray(img_with_res.get_image())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "As it was discussed above, Detectron2 provides generic approach for working with models for different use cases. The steps that required to convert and run models pretrained for Instance Segmentation use case will be very similar to Object Detection. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Instance Segmentation PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_name = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x\"\n",
    "model, cfg = get_model_and_config(model_name)\n",
    "sample_input = get_sample_inputs(image_file, cfg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Instance Segmentation Model to OpenVINO Intermediate Representation\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_xml_path = MODEL_DIR / (model_name.split(\"/\")[-1] + \".xml\")\n",
    "\n",
    "if not model_xml_path.exists():\n",
    "    ov_model = convert_detectron2_model(model, sample_input)\n",
    "    ov.save_model(ov_model, MODEL_DIR / (model_name.split(\"/\")[-1] + \".xml\"))\n",
    "else:\n",
    "    ov_model = model_xml_path"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Instance Segmentation model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "In comparison with Object Detection, Instance Segmentation models have additional output that represents instance masks for each object. Our postprocessing function handle this difference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "compiled_model = core.compile_model(ov_model, device.value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = compiled_model(sample_input[0][\"image\"])\n",
    "results = postprocess_detection_result(results, sample_input[0][\"height\"], sample_input[0][\"width\"], conf_threshold=0.05)\n",
    "img_with_res = draw_instance_prediction(np.array(image), results, cfg)\n",
    "Image.fromarray(img_with_res.get_image())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/c4dee890-6a18-4c45-8423-809653c85cb0",
   "tags": {
    "categories": [
     "Convert"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection",
     "Image Segmentation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
